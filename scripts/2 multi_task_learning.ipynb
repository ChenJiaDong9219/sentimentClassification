{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "datafile = open('./data.pkl', 'rb')\n",
    "# X_test will not be used\n",
    "X_train = pickle.load(datafile)\n",
    "X_test = pickle.load(datafile)\n",
    "word_to_index = pickle.load(datafile)\n",
    "index_to_word = pickle.load(datafile)\n",
    "word_to_vec_map = pickle.load(datafile)\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 定义AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二元分类的 AUC 的计算方式\n",
    "def auc(y_true, y_pred):\n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    N = K.sum(1 - y_true)\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    P = K.sum(y_true)\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 创建嵌入层，加载预训练的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用的预训练的 word embedding 是 40 万个单词的训练结果，它们的特征维数是 50\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    创建一个 Keras 的 Embedding() 层，并且加载之前已经训练好的 embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    # 词典中单词的个数+1，+1是 keras 模型的训练要求\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    # 获取单词的特征维数，随便找个单词就行了\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    \n",
    "    # 将 embedding 矩阵初始化为全 0 的，大小为 (vocab_len, emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # 将 emb_matrix 的行号当做单词的编号，然后将这个单词的 embedding 放到这一行，这样就把预训练的 embedding 加载进来了\n",
    "    # 注意，由于单词编号是从 1 开始的，所以行 0 是没有 embedding 的，这就是为什么前面要 +1\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # 创建 Keras 的 Embedding 层\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=True)\n",
    "\n",
    "    # build embedding layer，在设置 embedding layer 的权重的时候，这一步是必须的\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # 将 emb_matrix 设置为 embedding_layer 的权重。\n",
    "    # 到这里为止我们就创建了一个预训练好的 embedding layer\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 word embedding 层\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 创建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建共享部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其他所有的分类模型可以基于这个函数进行创建\n",
    "def shared_model(input_shape, word_to_vec_map, word_to_index, emb_layer):\n",
    "    \"\"\"\n",
    "    返回：一个 Keras 的模型\n",
    "    \n",
    "    参数:\n",
    "    input_shape -- MAX_COMMENT_TEXT_SEQ\n",
    "    word_to_vec_map\n",
    "    word_to_index\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_layer = emb_layer\n",
    "    # 创建输入层，输入的是句子的单词编号列表\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "    # 句子编号列表进入 embedding_layer 之后会返回对应的 embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    dr_r = 0.5\n",
    "    \n",
    "    X = BatchNormalization()(embeddings)\n",
    "    X = LSTM(32, return_sequences=True)(X)\n",
    "    X = Dropout(dr_r)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X, _, __ = LSTM(32, return_state = True)(X)\n",
    "    X = Dropout(dr_r)(X)\n",
    "    \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(8, activation='relu')(X)\n",
    "    X = Dropout(dr_r)(X)\n",
    "    \n",
    "    X = BatchNormalization()(X)\n",
    "#     X = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "#     model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    \n",
    "    return sentence_indices, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COMMENT_TEXT_SEQ = 200\n",
    "sentence_indices, shared_parts = shared_model((MAX_COMMENT_TEXT_SEQ, ), word_to_vec_map, word_to_index, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建多任务模型\n",
    "参考链接：https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_dense = Dense(10, activation='relu')(shared_parts)\n",
    "toxic_output = Dense(1, activation='sigmoid')(toxic_dense)\n",
    "\n",
    "severe_toxic_dense = Dense(10, activation='relu')(shared_parts)\n",
    "severe_toxic_output = Dense(1, activation='sigmoid')(severe_toxic_dense)\n",
    "\n",
    "obscene_dense = Dense(10, activation='relu')(shared_parts)\n",
    "obscene_output = Dense(1, activation='sigmoid')(obscene_dense)\n",
    "\n",
    "threat_dense = Dense(10, activation='relu')(shared_parts)\n",
    "threat_output = Dense(1, activation='sigmoid')(threat_dense)\n",
    "\n",
    "insult_dense = Dense(10, activation='relu')(shared_parts)\n",
    "insult_output = Dense(1, activation='sigmoid')(insult_dense)\n",
    "\n",
    "identity_hate_dense = Dense(10, activation='relu')(shared_parts)\n",
    "identity_hate_output = Dense(1, activation='sigmoid')(identity_hate_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=sentence_indices, outputs=[toxic_output, \n",
    "                                                severe_toxic_output, \n",
    "                                                obscene_output, \n",
    "                                                threat_output, \n",
    "                                                insult_output, \n",
    "                                                identity_hate_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 148401 samples, validate on 11170 samples\n",
      "Epoch 1/10\n",
      "148401/148401 [==============================] - 3217s 22ms/step - loss: 4.1433 - dense_2_loss: 0.6728 - dense_3_loss: 0.7128 - dense_4_loss: 0.6802 - dense_5_loss: 0.7389 - dense_6_loss: 0.6569 - dense_7_loss: 0.6817 - dense_2_auc: 0.6174 - dense_3_auc: 0.6493 - dense_4_auc: 0.5487 - dense_5_auc: nan - dense_6_auc: 0.5719 - dense_7_auc: 0.5936 - val_loss: 2.8589 - val_dense_2_loss: 0.4319 - val_dense_3_loss: 0.5012 - val_dense_4_loss: 0.4905 - val_dense_5_loss: 0.4736 - val_dense_6_loss: 0.4853 - val_dense_7_loss: 0.4763 - val_dense_2_auc: 0.5313 - val_dense_3_auc: 0.8592 - val_dense_4_auc: 0.6491 - val_dense_5_auc: nan - val_dense_6_auc: 0.6281 - val_dense_7_auc: 0.6115\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.85888, saving model to ./models/model-01.h5\n",
      "Epoch 2/10\n",
      "148401/148401 [==============================] - 3801s 26ms/step - loss: 2.4665 - dense_2_loss: 0.4048 - dense_3_loss: 0.4467 - dense_4_loss: 0.3994 - dense_5_loss: 0.4249 - dense_6_loss: 0.4050 - dense_7_loss: 0.3856 - dense_2_auc: 0.6150 - dense_3_auc: 0.7305 - dense_4_auc: 0.6208 - dense_5_auc: nan - dense_6_auc: 0.6180 - dense_7_auc: 0.5932 - val_loss: 1.6622 - val_dense_2_loss: 0.3334 - val_dense_3_loss: 0.2848 - val_dense_4_loss: 0.2783 - val_dense_5_loss: 0.2515 - val_dense_6_loss: 0.2849 - val_dense_7_loss: 0.2293 - val_dense_2_auc: 0.4093 - val_dense_3_auc: 0.8650 - val_dense_4_auc: 0.5927 - val_dense_5_auc: nan - val_dense_6_auc: 0.5659 - val_dense_7_auc: 0.5844\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.85888 to 1.66223, saving model to ./models/model-02.h5\n",
      "Epoch 3/10\n",
      "148401/148401 [==============================] - 3954s 27ms/step - loss: 1.4334 - dense_2_loss: 0.2895 - dense_3_loss: 0.2424 - dense_4_loss: 0.2325 - dense_5_loss: 0.2215 - dense_6_loss: 0.2397 - dense_7_loss: 0.2077 - dense_2_auc: 0.5834 - dense_3_auc: 0.7775 - dense_4_auc: 0.6604 - dense_5_auc: nan - dense_6_auc: 0.6246 - dense_7_auc: 0.5984 - val_loss: 1.0208 - val_dense_2_loss: 0.2595 - val_dense_3_loss: 0.1476 - val_dense_4_loss: 0.1712 - val_dense_5_loss: 0.1319 - val_dense_6_loss: 0.1812 - val_dense_7_loss: 0.1294 - val_dense_2_auc: 0.6232 - val_dense_3_auc: 0.9420 - val_dense_4_auc: 0.7836 - val_dense_5_auc: nan - val_dense_6_auc: 0.7640 - val_dense_7_auc: 0.7568\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.66223 to 1.02076, saving model to ./models/model-03.h5\n",
      "Epoch 4/10\n",
      " 37888/148401 [======>.......................] - ETA: 47:58 - loss: 1.0803 - dense_2_loss: 0.2440 - dense_3_loss: 0.1667 - dense_4_loss: 0.1787 - dense_5_loss: 0.1500 - dense_6_loss: 0.1852 - dense_7_loss: 0.1556 - dense_2_auc: 0.6676 - dense_3_auc: 0.8247 - dense_4_auc: 0.7515 - dense_5_auc: 0.5330 - dense_6_auc: 0.7120 - dense_7_auc: 0.6476"
     ]
    }
   ],
   "source": [
    "model_dir = './models'\n",
    "filepath = model_dir + '/model-{epoch:02d}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',save_best_only=True, verbose=1)\n",
    "callbacks_list = [checkpoint]\n",
    "train_result = model.fit(X_train['comment_text'], \n",
    "            [train[['toxic']], train[['severe_toxic']], train[['obscene']], train[['threat']], train[['insult']], train[['identity_hate']]], \n",
    "            epochs=10, \n",
    "            batch_size=1024, \n",
    "            validation_split=0.07, \n",
    "            callbacks = callbacks_list,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_result.history['train_loss'])\n",
    "plt.plot(train_result.history['validation_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
